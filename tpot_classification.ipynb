{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tpot\n",
      "  Downloading TPOT-0.11.5-py3-none-any.whl (82 kB)\n",
      "\u001b[K     |████████████████████████████████| 82 kB 912 kB/s eta 0:00:011\n",
      "\u001b[?25hRequirement already satisfied: joblib>=0.13.2 in /Users/patsnap/anaconda2/lib/python3.7/site-packages (from tpot) (0.14.0)\n",
      "Collecting pandas>=0.24.2\n",
      "  Downloading pandas-1.0.4-cp37-cp37m-macosx_10_9_x86_64.whl (10.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 10.0 MB 7.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting update-checker>=0.16\n",
      "  Downloading update_checker-0.17-py2.py3-none-any.whl (7.0 kB)\n",
      "Collecting deap>=1.2\n",
      "  Downloading deap-1.3.1-cp37-cp37m-macosx_10_13_x86_64.whl (109 kB)\n",
      "\u001b[K     |████████████████████████████████| 109 kB 38.2 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: tqdm>=4.36.1 in /Users/patsnap/anaconda2/lib/python3.7/site-packages (from tpot) (4.41.1)\n",
      "Requirement already satisfied: scikit-learn>=0.22.0 in /Users/patsnap/anaconda2/lib/python3.7/site-packages (from tpot) (0.22)\n",
      "Collecting stopit>=1.1.1\n",
      "  Downloading stopit-1.1.2.tar.gz (18 kB)\n",
      "Requirement already satisfied: scipy>=1.3.1 in /Users/patsnap/anaconda2/lib/python3.7/site-packages (from tpot) (1.4.1)\n",
      "Collecting numpy>=1.16.3\n",
      "  Downloading numpy-1.18.5-cp37-cp37m-macosx_10_9_x86_64.whl (15.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 15.1 MB 804 kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil>=2.6.1 in /Users/patsnap/anaconda2/lib/python3.7/site-packages (from pandas>=0.24.2->tpot) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in /Users/patsnap/anaconda2/lib/python3.7/site-packages (from pandas>=0.24.2->tpot) (2019.3)\n",
      "Requirement already satisfied: requests>=2.3.0 in /Users/patsnap/anaconda2/lib/python3.7/site-packages (from update-checker>=0.16->tpot) (2.22.0)\n",
      "Requirement already satisfied: six>=1.5 in /Users/patsnap/.local/lib/python3.7/site-packages (from python-dateutil>=2.6.1->pandas>=0.24.2->tpot) (1.14.0)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /Users/patsnap/anaconda2/lib/python3.7/site-packages (from requests>=2.3.0->update-checker>=0.16->tpot) (1.24.2)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /Users/patsnap/anaconda2/lib/python3.7/site-packages (from requests>=2.3.0->update-checker>=0.16->tpot) (3.0.4)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /Users/patsnap/anaconda2/lib/python3.7/site-packages (from requests>=2.3.0->update-checker>=0.16->tpot) (2.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/patsnap/anaconda2/lib/python3.7/site-packages (from requests>=2.3.0->update-checker>=0.16->tpot) (2019.9.11)\n",
      "Building wheels for collected packages: stopit\n",
      "  Building wheel for stopit (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for stopit: filename=stopit-1.1.2-py3-none-any.whl size=11956 sha256=ee2126161dc1459a10614748ecb33426766f47ac3e178aec295b24701894c4ab\n",
      "  Stored in directory: /Users/patsnap/Library/Caches/pip/wheels/e2/d2/79/eaf81edb391e27c87f51b8ef901ecc85a5363dc96b8b8d71e3\n",
      "Successfully built stopit\n",
      "\u001b[31mERROR: tensorflow-transform 0.21.2 has requirement tensorflow<2.2,>=1.15, but you'll have tensorflow 2.2.0 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: tensorflow-serving-api 2.1.0 has requirement tensorflow~=2.1.0, but you'll have tensorflow 2.2.0 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: tensorflow-data-validation 0.21.5 has requirement pandas<1,>=0.24, but you'll have pandas 1.0.4 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: tensorflow-data-validation 0.21.5 has requirement scikit-learn<0.22,>=0.18, but you'll have scikit-learn 0.22 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: spacy-transformers 0.5.1 has requirement transformers<2.1.0,>=2.0.0, but you'll have transformers 2.10.0 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: phik 0.9.12 has requirement joblib>=0.14.1, but you'll have joblib 0.14.0 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: nlpre 2.1.1 has requirement spacy==2.1.0, but you'll have spacy 2.2.4 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: medacy 0.2.0 has requirement spacy==2.2.0, but you'll have spacy 2.2.4 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: medacy 0.2.0 has requirement torch==1.2.0, but you'll have torch 1.4.0 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: medacy 0.2.0 has requirement transformers==2.3.0, but you'll have transformers 2.10.0 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: farm 0.4.2 has requirement transformers==2.7.0, but you'll have transformers 2.10.0 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: deeppavlov 0.8.0 has requirement Cython==0.29.14, but you'll have cython 0.29.17 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: deeppavlov 0.8.0 has requirement numpy==1.18.0, but you'll have numpy 1.18.5 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: deeppavlov 0.8.0 has requirement pandas==0.25.3, but you'll have pandas 1.0.4 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: deeppavlov 0.8.0 has requirement scikit-learn==0.21.2, but you'll have scikit-learn 0.22 which is incompatible.\u001b[0m\n",
      "Installing collected packages: numpy, pandas, update-checker, deap, stopit, tpot\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.15.1\n",
      "    Uninstalling numpy-1.15.1:\n",
      "      Successfully uninstalled numpy-1.15.1\n",
      "  Attempting uninstall: pandas\n",
      "    Found existing installation: pandas 0.23.4\n",
      "    Uninstalling pandas-0.23.4:\n",
      "      Successfully uninstalled pandas-0.23.4\n",
      "Successfully installed deap-1.3.1 numpy-1.18.5 pandas-1.0.4 stopit-1.1.2 tpot-0.11.5 update-checker-0.17\n"
     ]
    }
   ],
   "source": [
    "!pip install tpot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tpot import TPOTClassifier\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 0.,  0.,  5., 13.,  9.,  1.,  0.,  0.,  0.,  0., 13., 15., 10.,\n",
       "         15.,  5.,  0.,  0.,  3., 15.,  2.,  0., 11.,  8.,  0.,  0.,  4.,\n",
       "         12.,  0.,  0.,  8.,  8.,  0.,  0.,  5.,  8.,  0.,  0.,  9.,  8.,\n",
       "          0.,  0.,  4., 11.,  0.,  1., 12.,  7.,  0.,  0.,  2., 14.,  5.,\n",
       "         10., 12.,  0.,  0.,  0.,  0.,  6., 13., 10.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0., 12., 13.,  5.,  0.,  0.,  0.,  0.,  0., 11., 16.,\n",
       "          9.,  0.,  0.,  0.,  0.,  3., 15., 16.,  6.,  0.,  0.,  0.,  7.,\n",
       "         15., 16., 16.,  2.,  0.,  0.,  0.,  0.,  1., 16., 16.,  3.,  0.,\n",
       "          0.,  0.,  0.,  1., 16., 16.,  6.,  0.,  0.,  0.,  0.,  1., 16.,\n",
       "         16.,  6.,  0.,  0.,  0.,  0.,  0., 11., 16., 10.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  4., 15., 12.,  0.,  0.,  0.,  0.,  3., 16., 15.,\n",
       "         14.,  0.,  0.,  0.,  0.,  8., 13.,  8., 16.,  0.,  0.,  0.,  0.,\n",
       "          1.,  6., 15., 11.,  0.,  0.,  0.,  1.,  8., 13., 15.,  1.,  0.,\n",
       "          0.,  0.,  9., 16., 16.,  5.,  0.,  0.,  0.,  0.,  3., 13., 16.,\n",
       "         16., 11.,  5.,  0.,  0.,  0.,  0.,  3., 11., 16.,  9.,  0.],\n",
       "        [ 0.,  0.,  7., 15., 13.,  1.,  0.,  0.,  0.,  8., 13.,  6., 15.,\n",
       "          4.,  0.,  0.,  0.,  2.,  1., 13., 13.,  0.,  0.,  0.,  0.,  0.,\n",
       "          2., 15., 11.,  1.,  0.,  0.,  0.,  0.,  0.,  1., 12., 12.,  1.,\n",
       "          0.,  0.,  0.,  0.,  0.,  1., 10.,  8.,  0.,  0.,  0.,  8.,  4.,\n",
       "          5., 14.,  9.,  0.,  0.,  0.,  7., 13., 13.,  9.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  1., 11.,  0.,  0.,  0.,  0.,  0.,  0.,  7.,  8.,\n",
       "          0.,  0.,  0.,  0.,  0.,  1., 13.,  6.,  2.,  2.,  0.,  0.,  0.,\n",
       "          7., 15.,  0.,  9.,  8.,  0.,  0.,  5., 16., 10.,  0., 16.,  6.,\n",
       "          0.,  0.,  4., 15., 16., 13., 16.,  1.,  0.,  0.,  0.,  0.,  3.,\n",
       "         15., 10.,  0.,  0.,  0.,  0.,  0.,  2., 16.,  4.,  0.,  0.]]),\n",
       " array([0, 1, 2, ..., 8, 9, 8]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "digits = load_digits()\n",
    "digits.data[0:5], digits.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Optimization Progress', max=300.0, style=ProgressStyle(de…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generation 1 - Current best internal CV score: 0.9873798705768966\n",
      "Generation 2 - Current best internal CV score: 0.9873798705768966\n",
      "Generation 3 - Current best internal CV score: 0.9873798705768966\n",
      "Generation 4 - Current best internal CV score: 0.9873798705768966\n",
      "Generation 5 - Current best internal CV score: 0.9873798705768966\n",
      "Best pipeline: KNeighborsClassifier(MLPClassifier(input_matrix, alpha=0.01, learning_rate_init=0.1), n_neighbors=1, p=2, weights=uniform)\n",
      "0.9844444444444445\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(digits.data, digits.target,\n",
    "                                                    train_size=0.75, test_size=0.25, random_state=42)\n",
    "\n",
    "tpot = TPOTClassifier(generations=5, population_size=50, verbosity=2, random_state=42)\n",
    "tpot.fit(X_train, y_train)\n",
    "print(tpot.score(X_test, y_test))\n",
    "tpot.export('tpot_digits_pipeline.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[5.1, 3.5, 1.4, 0.2],\n",
       "        [4.9, 3. , 1.4, 0.2],\n",
       "        [4.7, 3.2, 1.3, 0.2],\n",
       "        [4.6, 3.1, 1.5, 0.2],\n",
       "        [5. , 3.6, 1.4, 0.2]]),\n",
       " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()\n",
    "iris.data[0:5], iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((112, 4), (38, 4), (112,), (38,))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target,\n",
    "                                                    train_size=0.75, test_size=0.25)\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b70d9ec2431a4189b8cc46c74ddfa44d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Optimization Progress', style=ProgressStyle(description_w…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generation 1 - Current best internal CV score: 0.9822134387351777\n",
      "Generation 2 - Current best internal CV score: 0.9822134387351777\n",
      "2.01 minutes have elapsed. TPOT will close down.\n",
      "TPOT closed during evaluation in one generation.\n",
      "WARNING: TPOT may not provide a good pipeline if TPOT is stopped/interrupted in a early generation.\n",
      "\n",
      "\n",
      "TPOT closed prematurely. Will use the current best pipeline.\n",
      "\n",
      "Best pipeline: MLPClassifier(input_matrix, alpha=0.1, learning_rate_init=0.1)\n",
      "0.9736842105263158\n"
     ]
    }
   ],
   "source": [
    "tpot = TPOTClassifier(verbosity=2, max_time_mins=2)\n",
    "tpot.fit(X_train, y_train)\n",
    "print(tpot.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tpot.export('tpot_iris_pipeline.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load tpot_iris_pipeline.py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.kernel_approximation import RBFSampler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# NOTE: Make sure that the outcome column is labeled 'target' in the data file\n",
    "tpot_data = pd.read_csv('PATH/TO/DATA/FILE', sep='COLUMN_SEPARATOR', dtype=np.float64)\n",
    "features = tpot_data.drop('target', axis=1)\n",
    "training_features, testing_features, training_classes, testing_classes = \\\n",
    "            train_test_split(features, tpot_data['target'], random_state=None)\n",
    "\n",
    "exported_pipeline = make_pipeline(\n",
    "    RBFSampler(gamma=0.8500000000000001),\n",
    "    DecisionTreeClassifier(criterion=\"entropy\", max_depth=3, min_samples_leaf=4, min_samples_split=9)\n",
    ")\n",
    "\n",
    "exported_pipeline.fit(training_features, training_classes)\n",
    "results = exported_pipeline.predict(testing_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Optimization Progress', max=300.0, style=ProgressStyle(de…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generation 1 - Current best internal CV score: -12.183902552870293\n",
      "Generation 2 - Current best internal CV score: -12.12046613884387\n",
      "Generation 3 - Current best internal CV score: -12.12046613884387\n",
      "Generation 4 - Current best internal CV score: -12.109022605866423\n",
      "Generation 5 - Current best internal CV score: -12.109022605866423\n",
      "Best pipeline: XGBRegressor(input_matrix, learning_rate=0.1, max_depth=6, min_child_weight=3, n_estimators=100, nthread=1, objective=reg:squarederror, subsample=0.7000000000000001)\n",
      "-7.439535149197824\n"
     ]
    }
   ],
   "source": [
    "from tpot import TPOTRegressor\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "housing = load_boston()\n",
    "X_train, X_test, y_train, y_test = train_test_split(housing.data, housing.target,\n",
    "                                                    train_size=0.75, test_size=0.25, random_state=42)\n",
    "\n",
    "tpot = TPOTRegressor(generations=5, population_size=50, verbosity=2, random_state=42)\n",
    "tpot.fit(X_train, y_train)\n",
    "print(tpot.score(X_test, y_test))\n",
    "tpot.export('tpot_boston_pipeline.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from tpot.export_utils import set_param_recursive\n",
    "\n",
    "# NOTE: Make sure that the outcome column is labeled 'target' in the data file\n",
    "tpot_data = pd.read_csv('PATH/TO/DATA/FILE', sep='COLUMN_SEPARATOR', dtype=np.float64)\n",
    "features = tpot_data.drop('target', axis=1)\n",
    "training_features, testing_features, training_target, testing_target = \\\n",
    "            train_test_split(features, tpot_data['target'], random_state=42)\n",
    "\n",
    "# Average CV score on the training set was: -10.812040755234403\n",
    "exported_pipeline = make_pipeline(\n",
    "    PolynomialFeatures(degree=2, include_bias=False, interaction_only=False),\n",
    "    ExtraTreesRegressor(bootstrap=False, max_features=0.5, min_samples_leaf=2, min_samples_split=3, n_estimators=100)\n",
    ")\n",
    "# Fix random state for all the steps in exported pipeline\n",
    "set_param_recursive(exported_pipeline.steps, 'random_state', 42)\n",
    "\n",
    "exported_pipeline.fit(training_features, training_target)\n",
    "results = exported_pipeline.predict(testing_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Optimization Progress', max=110.0, style=ProgressStyle(de…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generation 1 - Current best internal CV score: 1.0\n",
      "Generation 2 - Current best internal CV score: 1.0\n",
      "Generation 3 - Current best internal CV score: 1.0\n",
      "Generation 4 - Current best internal CV score: 1.0\n",
      "Generation 5 - Current best internal CV score: 1.0\n",
      "Generation 6 - Current best internal CV score: 1.0\n",
      "Generation 7 - Current best internal CV score: 1.0\n",
      "Generation 8 - Current best internal CV score: 1.0\n",
      "Generation 9 - Current best internal CV score: 1.0\n",
      "Generation 10 - Current best internal CV score: 1.0\n",
      "Best pipeline: PytorchLRClassifier(PCA(SelectPercentile(input_matrix, percentile=34), iterated_power=7, svd_solver=randomized), batch_size=16, learning_rate=1.0, num_epochs=10, weight_decay=0.0001)\n",
      "1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/patsnap/anaconda2/lib/python3.7/site-packages/sklearn/utils/validation.py:933: FutureWarning: Passing attributes to check_is_fitted is deprecated and will be removed in 0.23. The attributes argument is ignored.\n",
      "  \"argument is ignored.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from tpot import TPOTClassifier\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X, y = make_blobs(n_samples=100, centers=2, n_features=3, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.75, test_size=0.25)\n",
    "\n",
    "clf = TPOTClassifier(config_dict='TPOT NN', template='Selector-Transformer-PytorchLRClassifier', \n",
    "                     verbosity=2, population_size=10, generations=10)\n",
    "clf.fit(X_train, y_train)\n",
    "print(clf.score(X_test, y_test))\n",
    "tpot.export('tpot_nn_demo_pipeline.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
