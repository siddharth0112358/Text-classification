{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.3 64-bit ('anaconda3': virtualenv)",
   "metadata": {
    "interpreter": {
     "hash": "17f60a5376c77425fc92555942c601d199d1b6f4d9fe48b17446cfeffaeccd2f"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "https://www.kaggle.com/c/nlp-getting-started/data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"/Users/sdeshpande/Desktop/text_analysis_scripts/nlp-getting-started/train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(7613, 5)"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target  \n",
       "0       1  \n",
       "1       1  \n",
       "2       1  \n",
       "3       1  \n",
       "4       1  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>keyword</th>\n      <th>location</th>\n      <th>text</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Our Deeds are the Reason of this #earthquake M...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>4</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Forest fire near La Ronge Sask. Canada</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>5</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>All residents asked to 'shelter in place' are ...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>6</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>13,000 people receive #wildfires evacuation or...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>7</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.fillna(\"\", inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['total_text'] = train_df['keyword'] + train_df['location'] + train_df['text']\n",
    "train_df['total_text'] = train_df['total_text'].apply(lambda x: x.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1                   Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4                              Forest fire near La Ronge Sask. Canada   \n",
       "2   5                   All residents asked to 'shelter in place' are ...   \n",
       "3   6                   13,000 people receive #wildfires evacuation or...   \n",
       "4   7                   Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target                                         total_text  \n",
       "0       1  Our Deeds are the Reason of this #earthquake M...  \n",
       "1       1             Forest fire near La Ronge Sask. Canada  \n",
       "2       1  All residents asked to 'shelter in place' are ...  \n",
       "3       1  13,000 people receive #wildfires evacuation or...  \n",
       "4       1  Just got sent this photo from Ruby #Alaska as ...  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>keyword</th>\n      <th>location</th>\n      <th>text</th>\n      <th>target</th>\n      <th>total_text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td></td>\n      <td></td>\n      <td>Our Deeds are the Reason of this #earthquake M...</td>\n      <td>1</td>\n      <td>Our Deeds are the Reason of this #earthquake M...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>4</td>\n      <td></td>\n      <td></td>\n      <td>Forest fire near La Ronge Sask. Canada</td>\n      <td>1</td>\n      <td>Forest fire near La Ronge Sask. Canada</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>5</td>\n      <td></td>\n      <td></td>\n      <td>All residents asked to 'shelter in place' are ...</td>\n      <td>1</td>\n      <td>All residents asked to 'shelter in place' are ...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>6</td>\n      <td></td>\n      <td></td>\n      <td>13,000 people receive #wildfires evacuation or...</td>\n      <td>1</td>\n      <td>13,000 people receive #wildfires evacuation or...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>7</td>\n      <td></td>\n      <td></td>\n      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n      <td>1</td>\n      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df[['id','total_text','target']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                           total_text  target\n",
       "id                                                           \n",
       "1   Our Deeds are the Reason of this #earthquake M...       1\n",
       "4              Forest fire near La Ronge Sask. Canada       1\n",
       "5   All residents asked to 'shelter in place' are ...       1\n",
       "6   13,000 people receive #wildfires evacuation or...       1\n",
       "7   Just got sent this photo from Ruby #Alaska as ...       1"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>total_text</th>\n      <th>target</th>\n    </tr>\n    <tr>\n      <th>id</th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1</th>\n      <td>Our Deeds are the Reason of this #earthquake M...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Forest fire near La Ronge Sask. Canada</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>All residents asked to 'shelter in place' are ...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>13,000 people receive #wildfires evacuation or...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "train_df.set_index('id', inplace = True)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install --upgrade transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from simpletransformers.classification import ClassificationModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_lemmatizer(text):\n",
    "    lem_text = [WordNetLemmatizer().lemmatize(i) for i in text]\n",
    "    return lem_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def  clean_text(df, text_field, new_text_field_name):\n",
    "    df[new_text_field_name] = df[text_field].str.lower()\n",
    "    df[new_text_field_name] = df[new_text_field_name].apply(lambda elem: re.sub(r\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)|^rt|http.+?\", \"\", elem))  \n",
    "    df[new_text_field_name] = df[new_text_field_name].apply(lambda elem: re.sub(r\"\\d+\", \"\", elem))\n",
    "    df[new_text_field_name] = df[new_text_field_name].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n",
    "    df[new_text_field_name] = df[new_text_field_name].apply(lambda x: word_tokenize(x))\n",
    "    df[new_text_field_name] = df[new_text_field_name].apply(lambda x: word_lemmatizer(x))\n",
    "    df[new_text_field_name] = df[new_text_field_name].apply(lambda x: ' '.join(x))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_clean_df = clean_text(train_df, 'total_text', 'total_text_clean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                     total_text_clean  target\n",
       "id                                                           \n",
       "1          deed reason earthquake may allah forgive u       1\n",
       "4               forest fire near la ronge sask canada       1\n",
       "5   resident asked shelter place notified officer ...       1\n",
       "6   people receive wildfire evacuation order calif...       1\n",
       "7   got sent photo ruby alaska smoke wildfire pour...       1"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>total_text_clean</th>\n      <th>target</th>\n    </tr>\n    <tr>\n      <th>id</th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1</th>\n      <td>deed reason earthquake may allah forgive u</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>forest fire near la ronge sask canada</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>resident asked shelter place notified officer ...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>people receive wildfire evacuation order calif...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>got sent photo ruby alaska smoke wildfire pour...</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "source": [
    "train_clean_df = train_clean_df[['total_text_clean','target']]\n",
    "train_clean_df.head()"
   ]
  },
  {
   "source": [
    "https://simpletransformers.ai/docs/usage/"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_args = ClassificationArgs()\n",
    "model_args.num_train_epochs = 5\n",
    "model_args.learning_rate = 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Create a ClassificationModel\n",
    "model = ClassificationModel(\"roberta\", \"roberta-base\", use_cuda=False, args=model_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 7613/7613 [00:11<00:00, 657.37it/s]\n",
      "Epoch 1 of 3:   0%|          | 0/3 [00:00<?, ?it/s]\n",
      "Running Epoch 0 of 3:   0%|          | 0/952 [00:00<?, ?it/s]\u001b[A\n",
      "Epochs 0/3. Running Loss:    0.5312:   0%|          | 0/952 [00:02<?, ?it/s]\u001b[A\n",
      "Epochs 0/3. Running Loss:    0.5312:   0%|          | 1/952 [00:06<1:45:31,  6.66s/it]\u001b[A\n",
      "Epochs 0/3. Running Loss:    0.5851:   0%|          | 1/952 [00:08<1:45:31,  6.66s/it]\u001b[A\n",
      "Epochs 0/3. Running Loss:    0.5851:   0%|          | 2/952 [00:13<1:45:23,  6.66s/it]\u001b[A\n",
      "Epochs 0/3. Running Loss:    0.6649:   0%|          | 2/952 [00:15<1:45:23,  6.66s/it]\u001b[A\n",
      "Epochs 0/3. Running Loss:    0.6649:   0%|          | 3/952 [00:20<1:46:24,  6.73s/it]\u001b[A\n",
      "Epochs 0/3. Running Loss:    0.5160:   0%|          | 3/952 [00:22<1:46:24,  6.73s/it]\u001b[A\n",
      "Epochs 0/3. Running Loss:    0.5160:   0%|          | 4/952 [00:27<1:47:46,  6.82s/it]\u001b[A\n",
      "Epochs 0/3. Running Loss:    0.6785:   0%|          | 4/952 [00:29<1:47:46,  6.82s/it]\u001b[A\n",
      "Epochs 0/3. Running Loss:    0.6785:   1%|          | 5/952 [00:33<1:47:38,  6.82s/it]\u001b[A\n",
      "Epochs 0/3. Running Loss:    0.7460:   1%|          | 5/952 [00:36<1:47:38,  6.82s/it]\u001b[A\n",
      "Epochs 0/3. Running Loss:    0.7460:   1%|          | 6/952 [00:40<1:47:06,  6.79s/it]\u001b[A\n",
      "Epochs 0/3. Running Loss:    0.6452:   1%|          | 6/952 [00:42<1:47:06,  6.79s/it]\u001b[A\n",
      "Epochs 0/3. Running Loss:    0.6452:   1%|          | 7/952 [00:47<1:46:29,  6.76s/it]\u001b[A\n",
      "Epochs 0/3. Running Loss:    0.7642:   1%|          | 7/952 [00:49<1:46:29,  6.76s/it]\u001b[A\n",
      "Epochs 0/3. Running Loss:    0.7642:   1%|          | 8/952 [00:54<1:46:05,  6.74s/it]\u001b[A\n",
      "Epochs 0/3. Running Loss:    0.7883:   1%|          | 8/952 [00:56<1:46:05,  6.74s/it]\u001b[A\n",
      "Epochs 0/3. Running Loss:    0.7883:   1%|          | 9/952 [01:00<1:45:48,  6.73s/it]\u001b[A\n",
      "Epochs 0/3. Running Loss:    0.5963:   1%|          | 9/952 [01:02<1:45:48,  6.73s/it]\u001b[A\n",
      "Epochs 0/3. Running Loss:    0.5963:   1%|          | 10/952 [01:07<1:45:32,  6.72s/it]\u001b[A\n",
      "Epochs 0/3. Running Loss:    0.5796:   1%|          | 10/952 [01:09<1:45:32,  6.72s/it]\u001b[A\n",
      "Epochs 0/3. Running Loss:    0.5796:   1%|          | 11/952 [01:14<1:45:18,  6.71s/it]\u001b[A\n",
      "Epochs 0/3. Running Loss:    0.6178:   1%|          | 11/952 [01:16<1:45:18,  6.71s/it]\u001b[A\n",
      "Epochs 0/3. Running Loss:    0.6178:   1%|▏         | 12/952 [01:20<1:45:10,  6.71s/it]\u001b[A\n",
      "Epochs 0/3. Running Loss:    0.5146:   1%|▏         | 12/952 [01:22<1:45:10,  6.71s/it]\u001b[A\n",
      "Epochs 0/3. Running Loss:    0.5146:   1%|▏         | 13/952 [01:27<1:45:10,  6.72s/it]\u001b[A\n",
      "Epochs 0/3. Running Loss:    0.8148:   1%|▏         | 13/952 [01:29<1:45:10,  6.72s/it]\u001b[A\n",
      "Epochs 0/3. Running Loss:    0.8148:   1%|▏         | 14/952 [01:34<1:44:57,  6.71s/it]\u001b[A\n",
      "Epochs 0/3. Running Loss:    0.8320:   1%|▏         | 14/952 [01:36<1:44:57,  6.71s/it]\u001b[A\n",
      "Epochs 0/3. Running Loss:    0.8320:   2%|▏         | 15/952 [01:41<1:44:47,  6.71s/it]\u001b[A\n",
      "Epochs 0/3. Running Loss:    0.6527:   2%|▏         | 15/952 [01:43<1:44:47,  6.71s/it]\u001b[A\n",
      "Epochs 0/3. Running Loss:    0.6527:   2%|▏         | 16/952 [01:47<1:44:45,  6.72s/it]\u001b[A\n",
      "Epochs 0/3. Running Loss:    0.5776:   2%|▏         | 16/952 [01:49<1:44:45,  6.72s/it]\u001b[A\n",
      "Epochs 0/3. Running Loss:    0.5776:   2%|▏         | 17/952 [01:54<1:44:40,  6.72s/it]\u001b[A\n",
      "Epochs 0/3. Running Loss:    0.5324:   2%|▏         | 17/952 [01:56<1:44:40,  6.72s/it]\u001b[A\n",
      "Epochs 0/3. Running Loss:    0.5324:   2%|▏         | 18/952 [02:01<1:44:41,  6.73s/it]\u001b[A\n",
      "Epochs 0/3. Running Loss:    0.6784:   2%|▏         | 18/952 [02:03<1:44:41,  6.73s/it]\u001b[A\n",
      "Epochs 0/3. Running Loss:    0.6784:   2%|▏         | 19/952 [02:07<1:44:35,  6.73s/it]\u001b[A\n",
      "Epochs 0/3. Running Loss:    0.7619:   2%|▏         | 19/952 [02:10<1:44:35,  6.73s/it]\u001b[A\n",
      "Epochs 0/3. Running Loss:    0.7619:   2%|▏         | 20/952 [02:14<1:44:33,  6.73s/it]\u001b[A\n",
      "Epochs 0/3. Running Loss:    0.6920:   2%|▏         | 20/952 [02:16<1:44:33,  6.73s/it]\u001b[A\n",
      "Epochs 0/3. Running Loss:    0.6920:   2%|▏         | 21/952 [02:21<1:44:57,  6.76s/it]\u001b[A\n",
      "Epochs 0/3. Running Loss:    0.7618:   2%|▏         | 21/952 [02:23<1:44:57,  6.76s/it]\u001b[A\n",
      "Epochs 0/3. Running Loss:    0.7618:   2%|▏         | 22/952 [02:28<1:45:09,  6.78s/it]\u001b[A\n",
      "Epochs 0/3. Running Loss:    0.7390:   2%|▏         | 22/952 [02:30<1:45:09,  6.78s/it]\u001b[A\n",
      "Epochs 0/3. Running Loss:    0.7390:   2%|▏         | 23/952 [02:35<1:45:05,  6.79s/it]\u001b[A\n",
      "Epochs 0/3. Running Loss:    0.6341:   2%|▏         | 23/952 [02:37<1:45:05,  6.79s/it]\u001b[A\n",
      "Epochs 0/3. Running Loss:    0.6341:   3%|▎         | 24/952 [02:41<1:44:52,  6.78s/it]\u001b[A\n",
      "Epochs 0/3. Running Loss:    0.5785:   3%|▎         | 24/952 [02:43<1:44:52,  6.78s/it]\u001b[A\n",
      "Epochs 0/3. Running Loss:    0.5785:   3%|▎         | 25/952 [02:48<1:45:50,  6.85s/it]\u001b[A\n",
      "Epochs 0/3. Running Loss:    0.6777:   3%|▎         | 25/952 [02:51<1:45:50,  6.85s/it]\u001b[A\n",
      "Epochs 0/3. Running Loss:    0.6777:   3%|▎         | 26/952 [02:55<1:46:21,  6.89s/it]\u001b[A\n",
      "Epochs 0/3. Running Loss:    0.6376:   3%|▎         | 26/952 [02:57<1:46:21,  6.89s/it]\u001b[A\n",
      "Epochs 0/3. Running Loss:    0.6376:   3%|▎         | 27/952 [03:02<1:46:08,  6.88s/it]\u001b[A\n",
      "Epochs 0/3. Running Loss:    0.9293:   3%|▎         | 27/952 [03:04<1:46:08,  6.88s/it]\u001b[A\n",
      "Epochs 0/3. Running Loss:    0.9293:   3%|▎         | 28/952 [03:09<1:45:49,  6.87s/it]\u001b[A\n",
      "Epochs 0/3. Running Loss:    0.6257:   3%|▎         | 28/952 [03:11<1:45:49,  6.87s/it]\u001b[A\n",
      "Epochs 0/3. Running Loss:    0.6257:   3%|▎         | 29/952 [03:16<1:44:52,  6.82s/it]\u001b[A\n",
      "Epochs 0/3. Running Loss:    0.5982:   3%|▎         | 29/952 [03:18<1:44:52,  6.82s/it]\u001b[A\n",
      "Epochs 0/3. Running Loss:    0.5982:   3%|▎         | 30/952 [03:23<1:44:20,  6.79s/it]\u001b[A\n",
      "Epochs 0/3. Running Loss:    0.7192:   3%|▎         | 30/952 [03:25<1:44:20,  6.79s/it]\u001b[A\n",
      "Epochs 0/3. Running Loss:    0.7192:   3%|▎         | 31/952 [03:30<1:46:02,  6.91s/it]\u001b[A\n",
      "Epochs 0/3. Running Loss:    0.7469:   3%|▎         | 31/952 [03:32<1:46:02,  6.91s/it]\u001b[A\n",
      "Epochs 0/3. Running Loss:    0.7469:   3%|▎         | 32/952 [03:36<1:45:02,  6.85s/it]\u001b[A\n",
      "Epochs 0/3. Running Loss:    0.7888:   3%|▎         | 32/952 [03:39<1:45:02,  6.85s/it]\u001b[A\n",
      "Epochs 0/3. Running Loss:    0.7888:   3%|▎         | 33/952 [03:43<1:44:54,  6.85s/it]\u001b[A\n",
      "Epochs 0/3. Running Loss:    0.6945:   3%|▎         | 33/952 [03:45<1:44:54,  6.85s/it]\u001b[A\n",
      "Epochs 0/3. Running Loss:    0.6945:   4%|▎         | 34/952 [03:50<1:45:00,  6.86s/it]\u001b[A\n",
      "Epochs 0/3. Running Loss:    0.6289:   4%|▎         | 34/952 [03:57<1:46:44,  6.98s/it]\n",
      "Epoch 1 of 3:   0%|          | 0/3 [03:57<?, ?it/s]\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-ad28120c53f0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_clean_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/simpletransformers/classification/classification_model.py\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(self, train_df, multi_label, output_dir, show_running_loss, args, eval_df, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    410\u001b[0m         \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakedirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexist_ok\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    411\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 412\u001b[0;31m         global_step, training_details = self.train(\n\u001b[0m\u001b[1;32m    413\u001b[0m             \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    414\u001b[0m             \u001b[0moutput_dir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/simpletransformers/classification/classification_model.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, train_dataloader, output_dir, multi_label, show_running_loss, eval_df, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    667\u001b[0m                     \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    668\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 669\u001b[0;31m                     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    670\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    671\u001b[0m                 \u001b[0mtr_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m                 create_graph=create_graph)\n\u001b[0;32m--> 221\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    128\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    131\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m         allow_unreachable=True)  # allow_unreachable flag\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.train_model(train_clean_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv(\"/Users/sdeshpande/Desktop/text_analysis_scripts/nlp-getting-started/test.csv\")\n",
    "test_df.fillna(\"\", inplace = True)\n",
    "test_df['total_text'] = test_df['keyword'] + test_df['location'] + test_df['text']\n",
    "test_df['total_text'] = test_df['total_text'].apply(lambda x: x.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = test_df[['id','total_text']]\n",
    "test_df.set_index('id', inplace = True)\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_clean_df = clean_text(test_df, 'total_text', 'total_text_clean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_clean_df = test_clean_df[['total_text_clean']]\n",
    "test_clean_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_clean_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_list = test_clean_df.total_text_clean.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions, raw_outputs = model.predict(prediction_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_clean_df['predictions'] = predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_clean_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df = test_clean_df[['predictions']]\n",
    "submission_df.columns = ['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df.to_csv(\"submission_file.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Evaluate the model\n",
    "# result, model_outputs, wrong_predictions = model.eval_model(eval_df)"
   ]
  }
 ]
}